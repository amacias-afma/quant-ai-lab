{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b70f884e",
   "metadata": {},
   "source": [
    "# ðŸ“Š Quant Risk Lab: Hybrid Volatility Forecasting\n",
    "\n",
    "**Author:** Alvaro Macias, PhD  \n",
    "**Objective:** Benchmark Classical Econometrics (GARCH) against Deep Learning (LSTM) for Financial Risk Management.\n",
    "\n",
    "## 1\\. Theoretical Foundation\n",
    "\n",
    "### 1.1 The Problem: Latent Volatility\n",
    "\n",
    "In finance, volatility ($\\sigma_t$) is not directly observable. We only see prices ($P_t$). To train any model, we first need a statistical proxy for \"true\" variance.\n",
    "We use **Squared Log Returns** as our unbiased (noisy) estimator for daily variance:\n",
    "\n",
    "$$r_t = \\ln(\\frac{P_t}{P_{t-1}})$$\n",
    "$$\\hat{\\sigma}^2_t \\approx r_t^2$$\n",
    "\n",
    "### 1.2 Stationarity\n",
    "\n",
    "Most time-series models (GARCH) assume **Stationarity** (mean and variance do not drift forever). Prices are *not* stationary, but Log Returns usually *are*. We verify this using the **Augmented Dickey-Fuller (ADF)** test.\n",
    "\n",
    "  * **Null Hypothesis ($H_0$):** A Unit Root is present (Non-Stationary).\n",
    "  * **Alternate Hypothesis ($H_1$):** The series is Stationary.\n",
    "  * **Threshold:** We need p-value $< 0.05$ to proceed.\n",
    "\n",
    "-----\n",
    "\n",
    "## 2\\. Step 1: Data Ingestion & Validation\n",
    "\n",
    "*We implement the pipeline to fetch data and prove it is model-ready.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96090c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a21d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FUNCTION: Data Pipeline\n",
    "# ---------------------------------------------------------\n",
    "def get_market_data(ticker='SPY', start='2015-01-01'):\n",
    "    print(f\"Fetching data for {ticker}...\")\n",
    "    df = yf.download(ticker, start=start, progress=False)\n",
    "    \n",
    "    # 1. Log Returns\n",
    "    df['log_ret'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))\n",
    "    \n",
    "    # 2. Target: Squared Returns (Proxy for Variance)\n",
    "    df['target_variance'] = df['log_ret'] ** 2\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Execute\n",
    "data = get_market_data('SPY')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# VALIDATION: Stationarity Test\n",
    "# ---------------------------------------------------------\n",
    "result = adfuller(data['log_ret'])\n",
    "print(f\"ADF Statistic: {result[0]:.4f}\")\n",
    "print(f\"p-value: {result[1]:.4f}\")\n",
    "\n",
    "if result[1] < 0.05:\n",
    "    print(\"âœ… PASS: Series is Stationary.\")\n",
    "else:\n",
    "    print(\"âŒ FAIL: Series is Non-Stationary (Diff required).\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(data['log_ret'], alpha=0.5)\n",
    "plt.title(f\"Log Returns of SPY\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07008bf1",
   "metadata": {},
   "source": [
    "## 3\\. Step 2: The Benchmark - GARCH(1,1)\n",
    "\n",
    "### 3.1 The Math\n",
    "\n",
    "The **Generalized Autoregressive Conditional Heteroskedasticity (GARCH)** model assumes that today's variance is a weighted average of:\n",
    "\n",
    "1.  **Long-run Variance ($\\omega$)**\n",
    "2.  **Yesterday's Shock ($\\alpha \\epsilon^2_{t-1}$)**: The \"News\" component.\n",
    "3.  **Yesterday's Variance ($\\beta \\sigma^2_{t-1}$)**: The \"Memory\" component.\n",
    "\n",
    "$$\\sigma^2_t = \\omega + \\alpha \\epsilon^2_{t-1} + \\beta \\sigma^2_{t-1}$$\n",
    "\n",
    "**Stability Constraint:** $\\alpha + \\beta < 1$. If this sums to $>1$, the variance explodes, and the model is invalid.\n",
    "\n",
    "### 3.2 Implementation & \"Quant Audit\"\n",
    "\n",
    "We train the model and then run **Meucci's Invariants Check** to ensure the residuals are white noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "from scipy import stats\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MODEL: GARCH Training\n",
    "# ---------------------------------------------------------\n",
    "# Scale returns by 100 for optimizer stability\n",
    "returns_scaled = data['log_ret'] * 100 \n",
    "\n",
    "# Define GARCH(1,1) with Normal distribution\n",
    "model_garch = arch_model(returns_scaled, vol='Garch', p=1, o=0, q=1, dist='Normal')\n",
    "res_garch = model_garch.fit(disp='off')\n",
    "\n",
    "print(res_garch.summary())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# DIAGNOSTIC: Meucci's Checklist\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- MODEL AUDIT ---\")\n",
    "\n",
    "# 1. Stationarity Check\n",
    "params = res_garch.params\n",
    "persistence = params['alpha[1]'] + params['beta[1]']\n",
    "print(f\"Persistence (alpha+beta): {persistence:.4f} (< 1.0 is Stable)\")\n",
    "\n",
    "# 2. Independence of Residuals (Ljung-Box)\n",
    "# We test squared standardized residuals for remaining clusters\n",
    "std_resid = res_garch.std_resid\n",
    "lb_test = acorr_ljungbox(std_resid**2, lags=[10], return_df=True)\n",
    "lb_p = lb_test['lb_pvalue'].values[0]\n",
    "\n",
    "print(f\"Ljung-Box p-value: {lb_p:.4f} (> 0.05 indicates White Noise)\")\n",
    "if lb_p > 0.05:\n",
    "    print(\"âœ… PASS: Residuals are independent.\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: Residuals still have patterns (Model Underfitted).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df7732",
   "metadata": {},
   "source": [
    "## 4\\. Step 3: The Challenger - LSTM with QLIKE Loss\n",
    "\n",
    "### 4.1 Why MSE fails for Risk\n",
    "\n",
    "Standard Mean Squared Error (MSE) treats over-prediction and under-prediction symmetrically.\n",
    "\n",
    "  * **Over-predicting Vol:** You hedge too much (Costly).\n",
    "  * **Under-predicting Vol:** You blow up (Fatal).\n",
    "\n",
    "We use **QLIKE (Quasi-Likelihood)** loss, derived from the likelihood of a Gaussian process. It penalizes under-estimation heavily.\n",
    "\n",
    "$$Loss = \\ln(h_t) + \\frac{r_t^2}{h_t}$$\n",
    "\n",
    "  * $h_t$: Predicted Variance\n",
    "  * $r_t^2$: Realized Variance\n",
    "\n",
    "### 4.2 Implementation\n",
    "\n",
    "We implement the custom loss and the LSTM network using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb73a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# THEORY: The Custom Loss\n",
    "# ---------------------------------------------------------\n",
    "class QLIKE_Loss(nn.Module):\n",
    "    def forward(self, pred_var, target_sq_ret):\n",
    "        # Loss = log(h) + y^2/h\n",
    "        loss = torch.log(pred_var) + (target_sq_ret / pred_var)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MODEL: LSTM Architecture\n",
    "# ---------------------------------------------------------\n",
    "class VolatilityLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VolatilityLSTM, self).__init__()\n",
    "        # Input: 1 feature (Log Return)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "        self.activation = nn.Softplus() # Ensures Variance > 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last_step = out[:, -1, :] # We only want the forecast for T+1\n",
    "        var_pred = self.fc(last_step)\n",
    "        return self.activation(var_pred)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PIPELINE: Training Loop\n",
    "# ---------------------------------------------------------\n",
    "# Prepare Data Sequences\n",
    "seq_length = 60\n",
    "X_list, y_list = [], []\n",
    "ret_vals = (data['log_ret'].values * 100) # Scaled\n",
    "\n",
    "for i in range(len(ret_vals) - seq_length):\n",
    "    X_list.append(ret_vals[i:i+seq_length])\n",
    "    y_list.append(ret_vals[i+seq_length]**2) # Target is NEXT day variance\n",
    "\n",
    "X_t = torch.tensor(X_list, dtype=torch.float32).unsqueeze(-1)\n",
    "y_t = torch.tensor(y_list, dtype=torch.float32)\n",
    "\n",
    "# Init Model\n",
    "model_lstm = VolatilityLSTM()\n",
    "criterion = QLIKE_Loss()\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=0.002)\n",
    "\n",
    "print(\"Training LSTM...\")\n",
    "losses = []\n",
    "for epoch in range(30): # Short run for demo\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_lstm(X_t)\n",
    "    loss = criterion(outputs.squeeze(), y_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"QLIKE Loss Descent\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01238092",
   "metadata": {},
   "source": [
    "## 5\\. Step 4: The Showdown (Backtest)\n",
    "\n",
    "We perform a rolling Out-of-Sample (OOS) evaluation.\n",
    "\n",
    "  * **GARCH:** Analytical forecast.\n",
    "  * **LSTM:** Neural inference.\n",
    "\n",
    "We compare them using **RMSE** (Accuracy) and **QLIKE** (Risk Safety)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f834b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# EVALUATION: Generate Forecasts\n",
    "# ---------------------------------------------------------\n",
    "# 1. LSTM Forecasts (In-Sample for simplicity of demo)\n",
    "model_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    pred_var_scaled = model_lstm(X_t).squeeze().numpy()\n",
    "\n",
    "# Rescale back to decimal variance\n",
    "pred_var_lstm = pred_var_scaled / (100**2)\n",
    "pred_vol_lstm = np.sqrt(pred_var_lstm) * np.sqrt(252) # Annualize\n",
    "\n",
    "# 2. GARCH Forecasts (Conditional Volatility from fit)\n",
    "# Align indices: GARCH returns 'cond_vol' for the same T. \n",
    "# We need to slice to match LSTM's \"sequence length\" loss\n",
    "garch_vol_raw = res_garch.conditional_volatility[seq_length:] \n",
    "garch_vol_ann = (garch_vol_raw / 100) * np.sqrt(252)\n",
    "\n",
    "# 3. The \"Truth\" (Realized Volatility Proxy)\n",
    "# We use a 21-day rolling window of squared returns for the \"Truth\" plot\n",
    "# to make it readable (pure squared returns are too noisy to view)\n",
    "true_sq_returns = data['log_ret'].values[seq_length:]**2\n",
    "true_vol_proxy = np.sqrt(pd.Series(true_sq_returns).rolling(21).mean()) * np.sqrt(252)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# VISUALIZATION\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Truth\n",
    "plt.plot(true_vol_proxy.values, color='black', label='Realized Vol (21d smooth)', linewidth=2)\n",
    "\n",
    "# Plot GARCH\n",
    "plt.plot(garch_vol_ann.values, color='red', label='GARCH(1,1)', alpha=0.7)\n",
    "\n",
    "# Plot LSTM\n",
    "plt.plot(pred_vol_lstm, color='blue', label='Deep LSTM (QLIKE)', alpha=0.7)\n",
    "\n",
    "plt.title(\"GARCH vs LSTM: Volatility Forecasting Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52470fc2",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "By observing the plot:\n",
    "\n",
    "1.  **GARCH** tends to react smoothly and is mean-reverting.\n",
    "2.  **LSTM** (with QLIKE) should react more aggressively to sudden spikes, as the loss function penalizes \"missing\" a crash."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant-ai-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
